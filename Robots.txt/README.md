# Why the Robots file is important

The robots.txt file, also known as the robots exclusion protocol or standard, is a text file that tells web robots (most often search engines) which pages on your site to crawl.


It also tells web robots which pages not to crawl.

The standard specifies how to inform the web robot about which areas of the website should not be processed or scanned. 
Robots are often used by search engines to categorize websites. Not all robots cooperate with the standard; email harvesters, spambots, malware and robots that scan for security vulnerabilities may even start with the portions of the website where they have been told to stay out. The standard can be used in conjunction with Sitemaps, a robot inclusion standard for websites. 


Letâ€™s say a search engine is about to visit a site. Before it visits the target page, it will check the robots.txt for instructions.
